{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3ee33880-5aa1-45e1-ba76-a97864cd0e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download (\"stopwords\")\n",
    "nltk.download (\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c50b5ecf-8b84-48e6-806b-8dea1e09562f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt', download_dir='C:/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b216df1e-c0b7-41ff-8ec0-69d8bf2ad5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9e0a7be8-5f67-4d4a-b380-09e7a7ca724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentProcessor:\n",
    "    \n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        self.documents = {}\n",
    "        self.doc_mapping = {}\n",
    "        \n",
    "    def load_documents(self):\n",
    "        print(f\"Loading the documents from: {self.directory_path}\")\n",
    "        doc_counter = 0\n",
    "        \n",
    "        for file in os.listdir(self.directory_path):\n",
    "            print(f\"Processing: {file}\")\n",
    "            if file.endswith(\".txt\"):\n",
    "                file_path = os.path.join(self.directory_path, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text_content = f.read()\n",
    "                    self.documents[doc_counter] = text_content\n",
    "                    self.doc_mapping[doc_counter] = file\n",
    "                    print(f\"Document ID {doc_counter} mapped to {file}\")\n",
    "                    doc_counter += 1\n",
    "        \n",
    "        print(f\" {len(self.documents)} Successfully loaded\")\n",
    "        return self.documents, self.doc_mapping\n",
    "    \n",
    "    def preprocess_text(self, text):\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "        tokens = word_tokenize(text)\n",
    "        processed_tokens = [lemmatizer.lemmatize(token) for token in tokens if len(token) > 1]\n",
    "        return processed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f3d47e8-381b-4da1-b6a9-6bca5d1e626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndexBuilder:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.index = defaultdict(set)\n",
    "        self.term_stats = Counter()\n",
    "    \n",
    "    def create_index(self, documents, processor):\n",
    "        for doc_id, text in documents.items():\n",
    "            tokens = processor.preprocess_text(text)\n",
    "            for token in tokens:\n",
    "                self.index[token].add(doc_id)\n",
    "                self.term_stats[token] += 1\n",
    "        \n",
    "        return self.index, self.term_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "232837bb-369b-482d-a7d5-54707576a264",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BooleanSearchEngine:\n",
    "    \n",
    "    def __init__(self, inverted_index, doc_mapping):\n",
    "        self.inverted_index = inverted_index\n",
    "        self.doc_mapping = doc_mapping\n",
    "    \n",
    "    def execute_query(self, query_string):\n",
    "        query_string = query_string.lower()\n",
    "        query_tokens = query_string.split()\n",
    "        \n",
    "        matching_docs = set()\n",
    "\n",
    "        operators = ['and', 'or', 'not']\n",
    "        search_terms = [token for token in query_tokens if token not in operators]\n",
    "        \n",
    "        if 'and' in query_tokens:\n",
    "            if all(term in self.inverted_index for term in search_terms):\n",
    "                matching_docs = self.inverted_index[search_terms[0]].copy()\n",
    "                for term in search_terms[1:]:\n",
    "                    matching_docs &= self.inverted_index[term]\n",
    "    \n",
    "        elif 'or' in query_tokens:\n",
    "            for term in search_terms:\n",
    "                if term in self.inverted_index:\n",
    "                    matching_docs |= self.inverted_index[term]\n",
    "        \n",
    "        elif 'not' in query_tokens:\n",
    "            excluded_term = query_tokens[1]\n",
    "            all_document_ids = set(self.doc_mapping.keys())\n",
    "            if excluded_term in self.inverted_index:\n",
    "                matching_docs = all_document_ids - self.inverted_index[excluded_term]\n",
    "            else:\n",
    "                matching_docs = all_document_ids\n",
    "        \n",
    "    \n",
    "        else:\n",
    "            if query_string in self.inverted_index:\n",
    "                matching_docs = self.inverted_index[query_string]\n",
    "    \n",
    "        result_files = [self.doc_mapping[doc_id] for doc_id in matching_docs \n",
    "                       if doc_id in self.doc_mapping]\n",
    "        \n",
    "        logging.info(f\"Query: '{query_string}' | Results: {result_files}\")\n",
    "        return result_files\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a150b318-e475-4b2f-ab15-d21ef8c96101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_file(term_statistics, output_path=\"queries.txt\", num_queries=5):\n",
    "    sample_queries = [\n",
    "        \"update AND feature\",\n",
    "        \"android OR window\",\n",
    "        \"NOT support\"\n",
    "    ]\n",
    "    \n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as query_file:\n",
    "        for q in sample_queries:\n",
    "            query_file.write(q + \"\\n\")\n",
    "    \n",
    "    print(f\"Query file created: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a6deb424-0652-4a76-906e-837fbd82c9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the documents from: C:\\nltk_data\n",
      "Processing: tokenizers\n",
      " 0 Successfully loaded\n",
      "Index preview: []\n",
      "Query file created: queries.txt\n",
      "Query: 'update AND feature' => Results: []\n",
      "\n",
      "Query: 'android OR window' => Results: []\n",
      "\n",
      "Query: 'NOT support' => Results: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_search_system():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    docs_folder = r\"C:\\nltk_data\"\n",
    "\n",
    "    doc_processor = DocumentProcessor(docs_folder)\n",
    "    documents, doc_mapping = doc_processor.load_documents()\n",
    "    \n",
    "    for doc_id, content in documents.items():\n",
    "        tokens = doc_processor.preprocess_text(content)\n",
    "        print(f\"Document {doc_id} preview:\", tokens[:20])\n",
    "    \n",
    "    index_builder = InvertedIndexBuilder()\n",
    "    inverted_index, term_stats = index_builder.create_index(documents, doc_processor)\n",
    "    print(\"Index preview:\", list(inverted_index.keys())[:20])\n",
    "   \n",
    "    create_query_file(term_stats)\n",
    "    \n",
    "  \n",
    "    search_engine = BooleanSearchEngine(inverted_index, doc_mapping)\n",
    "    test_queries = [\n",
    "        \"update AND feature\",\n",
    "        \"android OR window\",\n",
    "        \"NOT support\"\n",
    "    ]\n",
    "    \n",
    "\n",
    "    with open(\"search_results.txt\", 'w', encoding='utf-8') as output:\n",
    "        for query in test_queries:\n",
    "            results = search_engine.execute_query(query)\n",
    "            output_line = f\"Query: '{query}' => Results: {results}\\n\"\n",
    "            print(output_line)\n",
    "            output.write(output_line)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_search_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7dfa9-236d-4378-afcb-15294bb25c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05981e4c-5ad2-479c-834d-d818b114a9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
